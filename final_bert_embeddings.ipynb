{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0640dc4",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Text\n",
    "\n",
    "In today’s world, Twitter provides people with a way to publicly express their thoughts on any given subject in a concise, condensed format. This allows us to use tweets as a way to predict users’ thoughts or feelings on a certain subject.\n",
    "\n",
    "Since the 2016 U.S. election, the influence of social media on society has become more and more concerning. Fake news, hate speech, polarization, and echo chambers attract growing scholarships to pay attention to the discussions in the online space. Understanding the sentimental content on social media is crucial to further analysis\n",
    "\n",
    "In this project, we are going to compare and contrast two models on the performance of classifying a tweet based on sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d54aa",
   "metadata": {},
   "source": [
    "## Load data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b38e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vikramc18/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import your libraries here\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e17996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet = pd.read_csv('data/Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23763a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27479  ed167662a5                         But it was worth it  ****.   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "0                    I`d have responded, if I were going   neutral  \n",
       "1                                               Sooo SAD  negative  \n",
       "2                                            bullying me  negative  \n",
       "3                                         leave me alone  negative  \n",
       "4                                          Sons of ****,  negative  \n",
       "...                                                  ...       ...  \n",
       "27476                                             d lost  negative  \n",
       "27477                                      , don`t force  negative  \n",
       "27478                          Yay good for both of you.  positive  \n",
       "27479                         But it was worth it  ****.  positive  \n",
       "27480  All this flirting going on - The ATG smiles. Y...   neutral  \n",
       "\n",
       "[27481 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bd7b658b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-181-074fd2a55816>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-181-074fd2a55816>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df_tweet.loc[type(df_tweet['text']) !== str]\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df_tweet.loc[type(df_tweet['text']) !== str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4b5c9924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in df_tweet['text'].values:\n",
    "    if type(i) != str:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a942032",
   "metadata": {},
   "source": [
    "## BERTweet\n",
    "https://huggingface.co/docs/transformers/model_doc/bertweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bdff0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcbfc6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87246b81",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "- remove tweets classified as 'neutral' so that we can perform binary classification\n",
    "- remove non-string tweets\n",
    "    - possibly just map these to strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a14aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39275533/select-row-from-a-dataframe-based-on-the-type-of-the-objecti-e-str\n",
    "# df[df['A'].apply(lambda x: isinstance(x, str))]\n",
    "df_tweet_bert = df_tweet[df_tweet['text'].apply(lambda x: isinstance(x, str))].reset_index()\n",
    "df_tweet_bert = df_tweet_bert[df_tweet_bert['sentiment'] != 'neutral'].reset_index()\n",
    "#df_tweet_bert = df_tweet.loc[type(df_tweet['text']) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d99e8881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16358</th>\n",
       "      <td>27474</td>\n",
       "      <td>27475</td>\n",
       "      <td>b78ec00df5</td>\n",
       "      <td>enjoy ur night</td>\n",
       "      <td>enjoy</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16359</th>\n",
       "      <td>27475</td>\n",
       "      <td>27476</td>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16360</th>\n",
       "      <td>27476</td>\n",
       "      <td>27477</td>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16361</th>\n",
       "      <td>27477</td>\n",
       "      <td>27478</td>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16362</th>\n",
       "      <td>27478</td>\n",
       "      <td>27479</td>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16363 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       level_0  index      textID  \\\n",
       "0            1      1  549e992a42   \n",
       "1            2      2  088c60f138   \n",
       "2            3      3  9642c003ef   \n",
       "3            4      4  358bd9e861   \n",
       "4            6      6  6e0c6d75b1   \n",
       "...        ...    ...         ...   \n",
       "16358    27474  27475  b78ec00df5   \n",
       "16359    27475  27476  4eac33d1c0   \n",
       "16360    27476  27477  4f4c4fc327   \n",
       "16361    27477  27478  f67aae2310   \n",
       "16362    27478  27479  ed167662a5   \n",
       "\n",
       "                                                    text  \\\n",
       "0          Sooo SAD I will miss you here in San Diego!!!   \n",
       "1                              my boss is bullying me...   \n",
       "2                         what interview! leave me alone   \n",
       "3       Sons of ****, why couldn`t they put them on t...   \n",
       "4      2am feedings for the baby are fun when he is a...   \n",
       "...                                                  ...   \n",
       "16358                                     enjoy ur night   \n",
       "16359   wish we could come see u on Denver  husband l...   \n",
       "16360   I`ve wondered about rake to.  The client has ...   \n",
       "16361   Yay good for both of you. Enjoy the break - y...   \n",
       "16362                         But it was worth it  ****.   \n",
       "\n",
       "                    selected_text sentiment  \n",
       "0                        Sooo SAD  negative  \n",
       "1                     bullying me  negative  \n",
       "2                  leave me alone  negative  \n",
       "3                   Sons of ****,  negative  \n",
       "4                             fun  positive  \n",
       "...                           ...       ...  \n",
       "16358                       enjoy  positive  \n",
       "16359                      d lost  negative  \n",
       "16360               , don`t force  negative  \n",
       "16361   Yay good for both of you.  positive  \n",
       "16362  But it was worth it  ****.  positive  \n",
       "\n",
       "[16363 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cdcc309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "roberta_model = SentenceTransformer('paraphrase-distilroberta-base-v1');\n",
    "\n",
    "def normalize_encode_tweet(tweet):\n",
    "    norm = tokenizer.normalizeTweet(tweet)\n",
    "    encoded = roberta_model.encode(norm)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae70a52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16363/16363 [12:04<00:00, 22.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# show progress\n",
    "tqdm.pandas()\n",
    "\n",
    "# https://www.geeksforgeeks.org/create-a-new-column-in-pandas-dataframe-based-on-the-existing-columns/\n",
    "df_tweet_bert['embedding'] =  df_tweet_bert.progress_apply(lambda row: normalize_encode_tweet(row.text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b47ba34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tweet_bert.to_csv('tweet_roberta_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3fa8c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_bert = pd.read_csv('tweet_roberta_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b8127f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_bert.drop(df_tweet_bert.columns[[0, 1, 2]], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd7aeef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ 9.30877551e-02  4.43676770e-01  1.10505581e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>[-2.20891997e-01 -2.87244469e-02  1.46015704e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ 1.11802444e-02 -4.25624251e-01  1.02491967e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ 1.77452222e-01  2.84410834e-01  5.99784851e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "      <td>[-1.04325861e-01  2.68305153e-01 -1.53165251e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "1  088c60f138                          my boss is bullying me...   \n",
       "2  9642c003ef                     what interview! leave me alone   \n",
       "3  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "4  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "\n",
       "    selected_text sentiment                                          embedding  \n",
       "0        Sooo SAD  negative  [ 9.30877551e-02  4.43676770e-01  1.10505581e-...  \n",
       "1     bullying me  negative  [-2.20891997e-01 -2.87244469e-02  1.46015704e-...  \n",
       "2  leave me alone  negative  [ 1.11802444e-02 -4.25624251e-01  1.02491967e-...  \n",
       "3   Sons of ****,  negative  [ 1.77452222e-01  2.84410834e-01  5.99784851e-...  \n",
       "4             fun  positive  [-1.04325861e-01  2.68305153e-01 -1.53165251e-...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a59d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de697327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "# X = df_tweet_bert['embedding']\n",
    "# when reading BERT from csv\n",
    "X = df_tweet_bert['embedding'].apply(lambda s: ([float(x.strip(\" \\n\")) for x in s.strip(\"[]\").split()])).values.tolist()\n",
    "y = df_tweet_bert['sentiment'].map({'negative': 0, 'positive': 1}).values.tolist()\n",
    "\n",
    "# train + test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\n",
    "# re-split train to have training, validation, testing sets\n",
    "# https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state=1) # 0.2/0.8 = 0.25\n",
    "\n",
    "# train = 60%, val = 20%, test = 20% of original data\n",
    "# TODO: need higher proportion of training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d2b89",
   "metadata": {},
   "source": [
    "## Train Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51083884",
   "metadata": {},
   "source": [
    "### Data\n",
    "Reformat X and y so that they can be passed into neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d349324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "X_train_nn = np.asarray(X_train)\n",
    "y_train_nn = y_train\n",
    "y_train_nn = np.asarray([np.array(y) for y in y_train_nn])\n",
    "\n",
    "# use validation set in training\n",
    "X_val_nn = np.asarray(X_val)\n",
    "y_val_nn = y_val\n",
    "y_val_nn = np.asarray([np.array(y) for y in y_val_nn])\n",
    "validation_set = (X_val_nn, y_val_nn)\n",
    "\n",
    "# testing set\n",
    "X_test_nn = np.asarray(X_test)\n",
    "y_test_nn = y_test\n",
    "y_test_nn = np.asarray([np.array(y) for y in y_test_nn])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2498301",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "\n",
    "Hyperparameters experimented with:\n",
    "- Activation function for hidden layer\n",
    "- Number of nodes in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2984516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utility functions from Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac072906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "738da73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_nn(net):\n",
    "    \"\"\"\n",
    "    trains the given neural net on our training data\n",
    "    prints out model summary and metrics\n",
    "    \"\"\"\n",
    "    # fit training data and validaiton data\n",
    "    net.fit(X_train_nn, \n",
    "            y_train_nn, \n",
    "            epochs = 20, \n",
    "            batch_size = 10,\n",
    "            validation_data = validation_set,\n",
    "            verbose = 0)\n",
    "    # predict x_test values\n",
    "    y_pred_temp = net.predict(X_test_nn)\n",
    "    # convert from probabilities to 0/1\n",
    "    y_pred_net = [1 if y > 0.5 else 0 for y in y_pred_temp]\n",
    "    \n",
    "    # print metrics and net description\n",
    "    net_accuracy = accuracy_score(y_test_nn, y_pred_net)\n",
    "    net_precision = precision_score(y_test_nn, y_pred_net)\n",
    "    net_recall = recall_score(y_test_nn, y_pred_net)\n",
    "    net_f1 = f1_score(y_test_nn, y_pred_net)\n",
    "    \n",
    "    print(net.summary())    \n",
    "    print(f'Neural Net Accuracy:  {net_accuracy}')\n",
    "    print(f'Neural Net Precision: {net_precision}')\n",
    "    print(f'Neural Net Recall:    {net_recall}')\n",
    "    print(f'Neural Net F1 Score:  {net_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2893a139",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 874us/step\n",
      "Model: \"1_hidden_layer_with_128_nodes_sigmoid_relu_activation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (Dense)               (None, 256)               196864    \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 128)               32896     \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 229,889\n",
      "Trainable params: 229,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Neural Net Accuracy:  0.8900091659028414\n",
      "Neural Net Precision: 0.9193846153846154\n",
      "Neural Net Recall:    0.8670922809053976\n",
      "Neural Net F1 Score:  0.8924731182795699\n"
     ]
    }
   ],
   "source": [
    "# 1 hidden layer w/ 128 nodes and relu activation\n",
    "nn1 = Sequential(name='1_hidden_layer_with_128_nodes_sigmoid_relu_activation')\n",
    "nn1.add(Dense(256, input_shape=(768,), activation='sigmoid', name='input'))\n",
    "nn1.add(Dense(128, activation='relu', name='hidden'))\n",
    "nn1.add(Dense(1, activation='sigmoid', name='output'))\n",
    "nn1.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "train_evaluate_nn(nn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5a69633b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 858us/step\n",
      "Model: \"1_hidden_layer_with_128_nodes_sigmoid_sigmoid_activation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (Dense)               (None, 256)               196864    \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 128)               32896     \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 229,889\n",
      "Trainable params: 229,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Neural Net Accuracy:  0.8927589367552704\n",
      "Neural Net Precision: 0.9127557160048135\n",
      "Neural Net Recall:    0.8804410911201392\n",
      "Neural Net F1 Score:  0.8963072378138849\n"
     ]
    }
   ],
   "source": [
    "# 1 hidden layer w/ 128 nodes and sigmoid activation\n",
    "nn2 = Sequential(name='1_hidden_layer_with_128_nodes_sigmoid_sigmoid_activation')\n",
    "nn2.add(Dense(256, input_shape=(768,), activation='sigmoid', name='input'))\n",
    "nn2.add(Dense(128, activation='sigmoid', name='hidden'))\n",
    "nn2.add(Dense(1, activation='sigmoid', name='output'))\n",
    "nn2.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "train_evaluate_nn(nn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a7def0af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 949us/step\n",
      "Model: \"1_hidden_layer_with_128_nodes_sigmoid_softmax_activation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (Dense)               (None, 256)               196864    \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 128)               32896     \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 229,889\n",
      "Trainable params: 229,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Neural Net Accuracy:  0.5264283531927895\n",
      "Neural Net Precision: 0.5264283531927895\n",
      "Neural Net Recall:    1.0\n",
      "Neural Net F1 Score:  0.689751801441153\n"
     ]
    }
   ],
   "source": [
    "# 1 hidden layer w/ 128 nodes and softmax activation\n",
    "nn3 = Sequential(name='1_hidden_layer_with_128_nodes_sigmoid_softmax_activation')\n",
    "nn3.add(Dense(256, input_shape=(768,), activation='sigmoid', name='input'))\n",
    "nn3.add(Dense(128, activation='softmax', name='hidden'))\n",
    "nn3.add(Dense(1, activation='sigmoid', name='output'))\n",
    "nn3.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "train_evaluate_nn(nn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c8d51d0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 872us/step\n",
      "Model: \"1_hidden_layer_with_64_nodes_sigmoid_relu_activation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (Dense)               (None, 256)               196864    \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 64)                16448     \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Neural Net Accuracy:  0.8921478765658417\n",
      "Neural Net Precision: 0.9024676850763808\n",
      "Neural Net Recall:    0.8914683691236216\n",
      "Neural Net F1 Score:  0.8969343065693431\n"
     ]
    }
   ],
   "source": [
    "# 1 hidden layer w/ 64 nodes and relu activation\n",
    "nn4 = Sequential(name='1_hidden_layer_with_64_nodes_sigmoid_relu_activation')\n",
    "nn4.add(Dense(256, input_shape=(768,), activation='sigmoid', name='input'))\n",
    "nn4.add(Dense(64, activation='relu', name='hidden'))\n",
    "nn4.add(Dense(1, activation='sigmoid', name='output'))\n",
    "nn4.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "train_evaluate_nn(nn4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "78ded371",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 867us/step\n",
      "Model: \"1_hidden_layer_with_64_nodes_sigmoid_sigmoid_activation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (Dense)               (None, 256)               196864    \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 64)                16448     \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Neural Net Accuracy:  0.8854262144821264\n",
      "Neural Net Precision: 0.8842645381984037\n",
      "Neural Net Recall:    0.9001741149158444\n",
      "Neural Net F1 Score:  0.8921484037963762\n"
     ]
    }
   ],
   "source": [
    "# 1 hidden layer w/ 64 nodes and sigmoid activation\n",
    "nn5 = Sequential(name='1_hidden_layer_with_64_nodes_sigmoid_sigmoid_activation')\n",
    "nn5.add(Dense(256, input_shape=(768,), activation='sigmoid', name='input'))\n",
    "nn5.add(Dense(64, activation='sigmoid', name='hidden'))\n",
    "nn5.add(Dense(1, activation='sigmoid', name='output'))\n",
    "nn5.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "train_evaluate_nn(nn5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c80174f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 877us/step\n",
      "Model: \"1_hidden_layer_with_64_nodes_sigmoid_softmax_activation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (Dense)               (None, 256)               196864    \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 64)                16448     \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Neural Net Accuracy:  0.8918423464711274\n",
      "Neural Net Precision: 0.9275452841973766\n",
      "Neural Net Recall:    0.8618688334300638\n",
      "Neural Net F1 Score:  0.8935018050541516\n"
     ]
    }
   ],
   "source": [
    "# 1 hidden layer w/ 64 nodes and softmax activation\n",
    "nn6 = Sequential(name='1_hidden_layer_with_64_nodes_sigmoid_softmax_activation')\n",
    "nn6.add(Dense(256, input_shape=(768,), activation='sigmoid', name='input'))\n",
    "nn6.add(Dense(64, activation='softmax', name='hidden'))\n",
    "nn6.add(Dense(1, activation='sigmoid', name='output'))\n",
    "nn6.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "train_evaluate_nn(nn6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b892906",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "227cfa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f831efc",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "Increased `max_iter` for all models as needed to avoid convergence warnings\n",
    "\n",
    "Hyperparameters:\n",
    "- `solver`: the algorithm to use in the optimization process\n",
    "- `penalty`: norm of the penalty term - not all penalties can be used with all solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e7460d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_logistic_regression(model, description):\n",
    "    # train model\n",
    "    trained_model = model.fit(X_train, y_train)\n",
    "    # predict\n",
    "    y_pred = trained_model.predict(X_test)\n",
    "    \n",
    "    # metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(description)\n",
    "    print(f'Logistic Regression Accuracy:  {accuracy}')\n",
    "    print(f'Logistic Regression Precision: {precision}')\n",
    "    print(f'Logistic Regression Recall:    {recall}')\n",
    "    print(f'Logistic Regression F1 Score:  {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6fb0817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs, l2\n",
      "Logistic Regression Accuracy:  0.8875649251451267\n",
      "Logistic Regression Precision: 0.9016004742145821\n",
      "Logistic Regression Recall:    0.8827626233313988\n",
      "Logistic Regression F1 Score:  0.8920821114369502\n"
     ]
    }
   ],
   "source": [
    "# lbfgs, l2\n",
    "log_reg1 = LogisticRegression(max_iter=500,\n",
    "                              penalty='l2',\n",
    "                              solver='lbfgs',\n",
    "                              random_state=1)\n",
    "\n",
    "train_evaluate_logistic_regression(log_reg1, 'lbfgs, l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9cd2776a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs, none\n",
      "Logistic Regression Accuracy:  0.8802322028719829\n",
      "Logistic Regression Precision: 0.8944872554831061\n",
      "Logistic Regression Recall:    0.8757980266976204\n",
      "Logistic Regression F1 Score:  0.8850439882697947\n"
     ]
    }
   ],
   "source": [
    "# lbgfs, none\n",
    "log_reg2 = LogisticRegression(max_iter=500,\n",
    "                              penalty='none',\n",
    "                              solver='lbfgs',\n",
    "                              random_state=1)\n",
    "\n",
    "train_evaluate_logistic_regression(log_reg2, 'lbfgs, none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "007f2410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newton-cg, l2\n",
      "Logistic Regression Accuracy:  0.8878704552398411\n",
      "Logistic Regression Precision: 0.902135231316726\n",
      "Logistic Regression Recall:    0.8827626233313988\n",
      "Logistic Regression F1 Score:  0.8923437958345557\n"
     ]
    }
   ],
   "source": [
    "# newton-cg, l2\n",
    "log_reg3 = LogisticRegression(max_iter=500,\n",
    "                              penalty='l2',\n",
    "                              solver='newton-cg',\n",
    "                              random_state=1)\n",
    "\n",
    "train_evaluate_logistic_regression(log_reg3, 'newton-cg, l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4a7aa062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newton-cg, none\n",
      "Logistic Regression Accuracy:  0.8802322028719829\n",
      "Logistic Regression Precision: 0.8944872554831061\n",
      "Logistic Regression Recall:    0.8757980266976204\n",
      "Logistic Regression F1 Score:  0.8850439882697947\n"
     ]
    }
   ],
   "source": [
    "# newton-cg, none\n",
    "log_reg4 = LogisticRegression(max_iter=500,\n",
    "                              penalty='none',\n",
    "                              solver='newton-cg',\n",
    "                              random_state=1)\n",
    "\n",
    "train_evaluate_logistic_regression(log_reg4, 'newton-cg, none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "23a4cedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liblinear, l2\n",
      "Logistic Regression Accuracy:  0.8875649251451267\n",
      "Logistic Regression Precision: 0.9016004742145821\n",
      "Logistic Regression Recall:    0.8827626233313988\n",
      "Logistic Regression F1 Score:  0.8920821114369502\n"
     ]
    }
   ],
   "source": [
    "# liblinear, l2\n",
    "log_reg5 = LogisticRegression(max_iter=500,\n",
    "                              penalty='l2',\n",
    "                              solver='liblinear',\n",
    "                              random_state=1)\n",
    "\n",
    "train_evaluate_logistic_regression(log_reg5, 'liblinear, l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "be8493e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liblinear, l1\n",
      "Logistic Regression Accuracy:  0.8881759853345554\n",
      "Logistic Regression Precision: 0.9031491384432561\n",
      "Logistic Regression Recall:    0.8821822402785838\n",
      "Logistic Regression F1 Score:  0.8925425719318849\n"
     ]
    }
   ],
   "source": [
    "# liblinear, l1\n",
    "log_reg6 = LogisticRegression(max_iter=500,\n",
    "                              penalty='l1',\n",
    "                              solver='liblinear',\n",
    "                              random_state=1)\n",
    "\n",
    "train_evaluate_logistic_regression(log_reg6, 'liblinear, l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fba45941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sag, l2\n",
      "Logistic Regression Accuracy:  0.8875649251451267\n",
      "Logistic Regression Precision: 0.9016004742145821\n",
      "Logistic Regression Recall:    0.8827626233313988\n",
      "Logistic Regression F1 Score:  0.8920821114369502\n"
     ]
    }
   ],
   "source": [
    "# sag, l2\n",
    "log_reg7 = LogisticRegression(max_iter=500,\n",
    "                              penalty='l2',\n",
    "                              solver='sag',\n",
    "                              random_state=1)\n",
    "\n",
    "train_evaluate_logistic_regression(log_reg7, 'sag, l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d33fa267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sag, none\n",
      "Logistic Regression Accuracy:  0.8805377329666972\n",
      "Logistic Regression Precision: 0.895017793594306\n",
      "Logistic Regression Recall:    0.8757980266976204\n",
      "Logistic Regression F1 Score:  0.8853036080962159\n"
     ]
    }
   ],
   "source": [
    "# sag, none\n",
    "log_reg8 = LogisticRegression(max_iter=500,\n",
    "                              penalty='none',\n",
    "                              solver='sag',\n",
    "                              random_state=1)\n",
    "\n",
    "train_evaluate_logistic_regression(log_reg8, 'sag, none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c382fd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saga, l2\n",
      "Logistic Regression Accuracy:  0.8875649251451267\n",
      "Logistic Regression Precision: 0.9016004742145821\n",
      "Logistic Regression Recall:    0.8827626233313988\n",
      "Logistic Regression F1 Score:  0.8920821114369502\n"
     ]
    }
   ],
   "source": [
    "# saga, l2\n",
    "log_reg9 = LogisticRegression(max_iter=500,\n",
    "                              penalty='l2',\n",
    "                              solver='saga',\n",
    "                              random_state=1)\n",
    "\n",
    "train_evaluate_logistic_regression(log_reg9, 'saga, l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "752a58f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saga, l1\n",
      "Logistic Regression Accuracy:  0.8881759853345554\n",
      "Logistic Regression Precision: 0.9031491384432561\n",
      "Logistic Regression Recall:    0.8821822402785838\n",
      "Logistic Regression F1 Score:  0.8925425719318849\n"
     ]
    }
   ],
   "source": [
    "# saga, l1\n",
    "log_reg10 = LogisticRegression(max_iter=500,\n",
    "                               penalty='l1',\n",
    "                               solver='saga',\n",
    "                               random_state=1)\n",
    "\n",
    "train_evaluate_logistic_regression(log_reg10, 'saga, l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "409db9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saga, none\n",
      "Logistic Regression Accuracy:  0.8799266727772685\n",
      "Logistic Regression Precision: 0.8944246737841044\n",
      "Logistic Regression Recall:    0.8752176436448056\n",
      "Logistic Regression F1 Score:  0.8847169257846876\n"
     ]
    }
   ],
   "source": [
    "# saga, none\n",
    "log_reg11 = LogisticRegression(max_iter=800,\n",
    "                               penalty='none',\n",
    "                               solver='saga',\n",
    "                               random_state=1)\n",
    "\n",
    "train_evaluate_logistic_regression(log_reg11, 'saga, none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ccda3a65",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "l1_ratio must be between 0 and 1; got (l1_ratio=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-96477262daeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                random_state=1)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_evaluate_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_reg12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saga, elasticnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-0e6f1829c2a5>\u001b[0m in \u001b[0;36mtrain_evaluate_logistic_regression\u001b[0;34m(model, description)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_evaluate_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             if (not isinstance(self.l1_ratio, numbers.Number) or\n\u001b[1;32m   1313\u001b[0m                     self.l1_ratio < 0 or self.l1_ratio > 1):\n\u001b[0;32m-> 1314\u001b[0;31m                 raise ValueError(\"l1_ratio must be between 0 and 1;\"\n\u001b[0m\u001b[1;32m   1315\u001b[0m                                  \" got (l1_ratio=%r)\" % self.l1_ratio)\n\u001b[1;32m   1316\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_ratio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: l1_ratio must be between 0 and 1; got (l1_ratio=None)"
     ]
    }
   ],
   "source": [
    "# saga, elasticnet\n",
    "log_reg12 = LogisticRegression(max_iter=500,\n",
    "                               penalty='elasticnet',\n",
    "                               solver='saga',\n",
    "                               random_state=1)\n",
    "\n",
    "train_evaluate_logistic_regression(log_reg12, 'saga, elasticnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf300c2",
   "metadata": {},
   "source": [
    "# Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1fa24",
   "metadata": {},
   "source": [
    "Download the lexicon from http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar and extract it into `data/positive-words.txt` and `data/negative-words.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00150f",
   "metadata": {},
   "source": [
    "The following pre-processing steps are inspired from https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b17b55",
   "metadata": {},
   "source": [
    "We also pre-processed data so that it begins with < s> tokens (and ends with < /s> tokens). Inspired from answer: https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b0935",
   "metadata": {},
   "source": [
    "normalize text to regular expression\n",
    "code from https://gist.github.com/yamanahlawat/4443c6e9e65e74829dbb6b47dd81764a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387e180",
   "metadata": {},
   "source": [
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37554074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
