{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0640dc4",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Text\n",
    "\n",
    "In today’s world, Twitter provides people with a way to publicly express their thoughts on any given subject in a concise, condensed format. This allows us to use tweets as a way to predict users’ thoughts or feelings on a certain subject.\n",
    "\n",
    "Since the 2016 U.S. election, the influence of social media on society has become more and more concerning. Fake news, hate speech, polarization, and echo chambers attract growing scholarships to pay attention to the discussions in the online space. Understanding the sentimental content on social media is crucial to further analysis\n",
    "\n",
    "In this project, we are going to compare and contrast two models on the performance of classifying a tweet based on sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d54aa",
   "metadata": {},
   "source": [
    "## Load data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b38e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "# from nltk.stem import SnowballStemmer\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de6b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_patterns = [\n",
    "  (r'won\\'t', 'will not'),\n",
    "  (r'can\\'t', 'cannot'),\n",
    "  (r'i\\'m', 'i am'),\n",
    "  (r'ain\\'t', 'is not'),\n",
    "  (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "  (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "  (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "  (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "  (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "  (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in replacement_patterns]\n",
    "\n",
    "def replace(text):\n",
    "    s = text\n",
    "    for (pattern, repl) in patterns:\n",
    "        s = re.sub(pattern, repl, s)\n",
    "    return s\n",
    "\n",
    "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Process the paragram so it is tokenized into sentences.\n",
    "    To keep the nuance of social media, we are keeping the punctuation and forms of words.\n",
    "    \"\"\"\n",
    "    sent_text = nltk.sent_tokenize(text) # this gives us a list of sentences\n",
    "    \n",
    "    # now loop over each sentence and tokenize it separately\n",
    "    s = []\n",
    "    for sentence in sent_text:\n",
    "        # regualr expression\n",
    "        sentence = replace(sentence)\n",
    "        # tokenize sentence\n",
    "        tokenized_text = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
    "\n",
    "        s = s + tokenized_text\n",
    "    return s\n",
    "\n",
    "def process_data(series):\n",
    "    # returns text in this format:\n",
    "    # data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "    # \t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "    # \t\t\t['yet', 'another', 'sentence'],\n",
    "    # \t\t\t['one', 'more', 'sentence'],\n",
    "    # \t\t\t['and', 'the', 'final', 'sentence']]\n",
    "    tweets = []\n",
    "    for _,row in series.items():\n",
    "        tweets.append(process_text(str(row)))\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e17996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet = pd.read_csv('data/Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23763a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27479  ed167662a5                         But it was worth it  ****.   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "0                    I`d have responded, if I were going   neutral  \n",
       "1                                               Sooo SAD  negative  \n",
       "2                                            bullying me  negative  \n",
       "3                                         leave me alone  negative  \n",
       "4                                          Sons of ****,  negative  \n",
       "...                                                  ...       ...  \n",
       "27476                                             d lost  negative  \n",
       "27477                                      , don`t force  negative  \n",
       "27478                          Yay good for both of you.  positive  \n",
       "27479                         But it was worth it  ****.  positive  \n",
       "27480  All this flirting going on - The ATG smiles. Y...   neutral  \n",
       "\n",
       "[27481 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8044cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From notebook 11\n",
    "def load_lexicon(filename):\n",
    "    \"\"\"\n",
    "    Load a file from Bing Liu's sentiment lexicon\n",
    "    (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), containing\n",
    "    English words in Latin-1 encoding.\n",
    "    \n",
    "    One file contains a list of positive words, and the other contains\n",
    "    a list of negative words. The files contain comment lines starting\n",
    "    with ';' and blank lines, which should be skipped.\n",
    "    \"\"\"\n",
    "    lexicon = []\n",
    "    with open(filename, encoding='latin-1') as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip()\n",
    "            if line and not line.startswith(';'):\n",
    "                lexicon.append(line)\n",
    "    return lexicon\n",
    "\n",
    "pos_words = load_lexicon('data/positive-words.txt')\n",
    "neg_words = load_lexicon('data/negative-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17723d58",
   "metadata": {},
   "source": [
    "## Train the embeddings\n",
    "\n",
    "Right now using Glovec, can be changed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb5f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "02e707a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # From notebook 11\n",
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
    "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
    "    whether there is an initial line with the dimensions of the matrix.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
    "\n",
    "# for better performance, use the 42B data https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "embeddings = load_embeddings('data/glove.6B.50d.txt')\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "68998604",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vectors = embeddings.loc[embeddings.index.isin(pos_words)].dropna()\n",
    "neg_vectors = embeddings.loc[embeddings.index.isin(neg_words)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cc2b800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a942032",
   "metadata": {},
   "source": [
    "## BERTweet\n",
    "https://huggingface.co/docs/transformers/model_doc/bertweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bdff0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/class/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-12-04 22:31:41.434910: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbfc6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87246b81",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "- remove tweets classified as 'neutral' so that we can perform binary classification\n",
    "- remove non-string tweets\n",
    "    - possibly just map these to strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a14aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39275533/select-row-from-a-dataframe-based-on-the-type-of-the-objecti-e-str\n",
    "# df[df['A'].apply(lambda x: isinstance(x, str))]\n",
    "df_tweet_bert = df_tweet[df_tweet['text'].apply(lambda x: isinstance(x, str))].reset_index()\n",
    "df_tweet_bert = df_tweet_bert[df_tweet_bert['sentiment'] != 'neutral'].reset_index()\n",
    "#df_tweet_bert = df_tweet.loc[type(df_tweet['text']) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e8881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcc309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_encode_tweet(tweet):\n",
    "#     norm = tokenizer.normalizeTweet(tweet)\n",
    "#     encoded = tokenizer.encode(norm)\n",
    "#     return encoded\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "roberta_model = SentenceTransformer('paraphrase-distilroberta-base-v1');\n",
    "def normalize_encode_tweet(tweet):\n",
    "    norm = tokenizer.normalizeTweet(tweet)\n",
    "    encoded = roberta_model.encode(norm)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60417dfd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f6919279e647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# https://www.geeksforgeeks.org/create-a-new-column-in-pandas-dataframe-based-on-the-existing-columns/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_tweet_bert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdf_tweet_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnormalize_encode_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7763\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7764\u001b[0m         )\n\u001b[0;32m-> 7765\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-f6919279e647>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# https://www.geeksforgeeks.org/create-a-new-column-in-pandas-dataframe-based-on-the-existing-columns/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_tweet_bert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdf_tweet_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnormalize_encode_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-51912d026bea>\u001b[0m in \u001b[0;36mnormalize_encode_tweet\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mencoded_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbertweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         )\n\u001b[0;32m--> 851\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 )\n\u001b[1;32m    525\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    527\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/create-a-new-column-in-pandas-dataframe-based-on-the-existing-columns/\n",
    "# df_tweet_bert['embedding'] =  df_tweet_bert.apply(lambda row: normalize_encode_tweet(row.text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e32bb94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16363/16363 [12:04<00:00, 22.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# show progress\n",
    "tqdm.pandas()\n",
    "\n",
    "# https://www.geeksforgeeks.org/create-a-new-column-in-pandas-dataframe-based-on-the-existing-columns/\n",
    "df_tweet_bert['embedding'] =  df_tweet_bert.progress_apply(lambda row: normalize_encode_tweet(row.text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c3fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_bert.to_csv(\"tweet_roberta_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37603a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_bert = pd.read_csv(\"tweet_roberta_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9d8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_bert.drop(df_tweet_bert.columns[[0, 1, 2]], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8428c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ 9.30877551e-02  4.43676770e-01  1.10505581e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>[-2.20891997e-01 -2.87244469e-02  1.46015704e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ 1.11802444e-02 -4.25624251e-01  1.02491967e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ 1.77452222e-01  2.84410834e-01  5.99784851e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "      <td>[-1.04325861e-01  2.68305153e-01 -1.53165251e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "1  088c60f138                          my boss is bullying me...   \n",
       "2  9642c003ef                     what interview! leave me alone   \n",
       "3  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "4  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "\n",
       "    selected_text sentiment                                          embedding  \n",
       "0        Sooo SAD  negative  [ 9.30877551e-02  4.43676770e-01  1.10505581e-...  \n",
       "1     bullying me  negative  [-2.20891997e-01 -2.87244469e-02  1.46015704e-...  \n",
       "2  leave me alone  negative  [ 1.11802444e-02 -4.25624251e-01  1.02491967e-...  \n",
       "3   Sons of ****,  negative  [ 1.77452222e-01  2.84410834e-01  5.99784851e-...  \n",
       "4             fun  positive  [-1.04325861e-01  2.68305153e-01 -1.53165251e-...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5da695d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_tweet_bert['embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c12be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_tweet_bert['embedding'].apply(lambda x: x.strip(\"[]\").split()).reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "885513bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [9.30877551e-02, 4.43676770e-01, 1.10505581e-0...\n",
       "1    [-2.20891997e-01, -2.87244469e-02, 1.46015704e...\n",
       "2    [1.11802444e-02, -4.25624251e-01, 1.02491967e-...\n",
       "3    [1.77452222e-01, 2.84410834e-01, 5.99784851e-0...\n",
       "4    [-1.04325861e-01, 2.68305153e-01, -1.53165251e...\n",
       "Name: embedding, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "80db7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = [len(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ec2f85f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fbc6f7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a59d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa9486b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "de697327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "# X = df_tweet_bert['embedding']\n",
    "# when reading BERT from csv\n",
    "X = df_tweet_bert['embedding'].apply(lambda s: ([float(x.strip(\" \\n\")) for x in s.strip(\"[]\").split()])).values.tolist()\n",
    "y = df_tweet_bert['sentiment'].values.tolist()\n",
    "\n",
    "# train + test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X,dtype=object), np.array(y), test_size = 0.2, random_state=1)\n",
    "# re-split train to have training, validation, testing sets\n",
    "# https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state=1) # 0.2/0.8 = 0.25\n",
    "\n",
    "# train = 60%, val = 20%, test = 20% of original data\n",
    "# TODO: need higher proportion of training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "48d1359f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9817"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "265b52cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3273"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "802b6122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3273"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "43b1869f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fc7eee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ee53edc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093088</td>\n",
       "      <td>0.443677</td>\n",
       "      <td>0.110506</td>\n",
       "      <td>-0.342519</td>\n",
       "      <td>0.334282</td>\n",
       "      <td>0.134133</td>\n",
       "      <td>-0.079811</td>\n",
       "      <td>0.109643</td>\n",
       "      <td>-0.126565</td>\n",
       "      <td>0.068254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244574</td>\n",
       "      <td>0.369231</td>\n",
       "      <td>0.375811</td>\n",
       "      <td>0.209643</td>\n",
       "      <td>-0.253997</td>\n",
       "      <td>0.118141</td>\n",
       "      <td>-0.014578</td>\n",
       "      <td>0.338187</td>\n",
       "      <td>0.189731</td>\n",
       "      <td>-0.100272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.220892</td>\n",
       "      <td>-0.028724</td>\n",
       "      <td>0.146016</td>\n",
       "      <td>-0.145213</td>\n",
       "      <td>0.571359</td>\n",
       "      <td>-1.157924</td>\n",
       "      <td>0.182461</td>\n",
       "      <td>-0.330796</td>\n",
       "      <td>0.137217</td>\n",
       "      <td>0.456897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123236</td>\n",
       "      <td>-0.210848</td>\n",
       "      <td>0.203079</td>\n",
       "      <td>-0.147492</td>\n",
       "      <td>-0.024679</td>\n",
       "      <td>0.178868</td>\n",
       "      <td>0.277888</td>\n",
       "      <td>-0.279467</td>\n",
       "      <td>-0.325976</td>\n",
       "      <td>0.090958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011180</td>\n",
       "      <td>-0.425624</td>\n",
       "      <td>0.102492</td>\n",
       "      <td>-0.452972</td>\n",
       "      <td>-0.219800</td>\n",
       "      <td>-0.457667</td>\n",
       "      <td>0.314573</td>\n",
       "      <td>-0.382062</td>\n",
       "      <td>-0.002846</td>\n",
       "      <td>-0.448729</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.367298</td>\n",
       "      <td>-0.181243</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>-0.251850</td>\n",
       "      <td>-0.453673</td>\n",
       "      <td>0.200214</td>\n",
       "      <td>-0.296771</td>\n",
       "      <td>-0.072697</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>-0.251559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.177452</td>\n",
       "      <td>0.284411</td>\n",
       "      <td>0.059978</td>\n",
       "      <td>0.294704</td>\n",
       "      <td>-0.461114</td>\n",
       "      <td>-0.078591</td>\n",
       "      <td>0.027912</td>\n",
       "      <td>-0.055785</td>\n",
       "      <td>0.103038</td>\n",
       "      <td>-0.375385</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141730</td>\n",
       "      <td>0.011023</td>\n",
       "      <td>-0.028931</td>\n",
       "      <td>0.275780</td>\n",
       "      <td>0.392628</td>\n",
       "      <td>0.255509</td>\n",
       "      <td>-0.228360</td>\n",
       "      <td>-0.115353</td>\n",
       "      <td>-0.060927</td>\n",
       "      <td>-0.111012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.104326</td>\n",
       "      <td>0.268305</td>\n",
       "      <td>-0.153165</td>\n",
       "      <td>-0.098395</td>\n",
       "      <td>0.136013</td>\n",
       "      <td>-0.023756</td>\n",
       "      <td>0.088492</td>\n",
       "      <td>-0.139561</td>\n",
       "      <td>-0.056441</td>\n",
       "      <td>-0.164708</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252163</td>\n",
       "      <td>-0.168705</td>\n",
       "      <td>0.131596</td>\n",
       "      <td>0.134020</td>\n",
       "      <td>-0.135192</td>\n",
       "      <td>0.136525</td>\n",
       "      <td>0.146635</td>\n",
       "      <td>-0.221401</td>\n",
       "      <td>-0.097489</td>\n",
       "      <td>0.133560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16358</th>\n",
       "      <td>0.270939</td>\n",
       "      <td>-0.060949</td>\n",
       "      <td>0.430409</td>\n",
       "      <td>0.063049</td>\n",
       "      <td>0.087054</td>\n",
       "      <td>0.140644</td>\n",
       "      <td>-0.005588</td>\n",
       "      <td>-0.014261</td>\n",
       "      <td>-0.410215</td>\n",
       "      <td>-0.261869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041894</td>\n",
       "      <td>-0.191259</td>\n",
       "      <td>0.005462</td>\n",
       "      <td>0.223951</td>\n",
       "      <td>-0.096526</td>\n",
       "      <td>0.615836</td>\n",
       "      <td>0.522237</td>\n",
       "      <td>0.275737</td>\n",
       "      <td>0.338247</td>\n",
       "      <td>-0.043370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16359</th>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.125610</td>\n",
       "      <td>0.194917</td>\n",
       "      <td>-0.271436</td>\n",
       "      <td>-0.618391</td>\n",
       "      <td>-0.181236</td>\n",
       "      <td>-0.123064</td>\n",
       "      <td>0.412982</td>\n",
       "      <td>-0.064641</td>\n",
       "      <td>-0.461746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077754</td>\n",
       "      <td>-0.076400</td>\n",
       "      <td>-0.029041</td>\n",
       "      <td>0.303071</td>\n",
       "      <td>-0.178294</td>\n",
       "      <td>0.099713</td>\n",
       "      <td>0.020586</td>\n",
       "      <td>-0.142542</td>\n",
       "      <td>0.411557</td>\n",
       "      <td>-0.139364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16360</th>\n",
       "      <td>0.233436</td>\n",
       "      <td>-0.333714</td>\n",
       "      <td>0.249148</td>\n",
       "      <td>-0.065081</td>\n",
       "      <td>-0.009866</td>\n",
       "      <td>0.274120</td>\n",
       "      <td>-0.096856</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.101609</td>\n",
       "      <td>-0.074891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064156</td>\n",
       "      <td>0.152942</td>\n",
       "      <td>0.003013</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>-0.168976</td>\n",
       "      <td>0.035944</td>\n",
       "      <td>-0.074668</td>\n",
       "      <td>-0.267087</td>\n",
       "      <td>-0.333665</td>\n",
       "      <td>0.200270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16361</th>\n",
       "      <td>0.060705</td>\n",
       "      <td>0.650956</td>\n",
       "      <td>0.364366</td>\n",
       "      <td>-0.137743</td>\n",
       "      <td>0.125858</td>\n",
       "      <td>0.377062</td>\n",
       "      <td>0.200909</td>\n",
       "      <td>-0.164970</td>\n",
       "      <td>0.192967</td>\n",
       "      <td>0.105727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133314</td>\n",
       "      <td>-0.485834</td>\n",
       "      <td>-0.629914</td>\n",
       "      <td>0.162492</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>0.006419</td>\n",
       "      <td>0.090930</td>\n",
       "      <td>0.374194</td>\n",
       "      <td>0.108635</td>\n",
       "      <td>0.091865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16362</th>\n",
       "      <td>0.397758</td>\n",
       "      <td>0.152827</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.076836</td>\n",
       "      <td>-0.368298</td>\n",
       "      <td>-0.017233</td>\n",
       "      <td>0.010598</td>\n",
       "      <td>-0.227857</td>\n",
       "      <td>-0.081790</td>\n",
       "      <td>-0.110883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380262</td>\n",
       "      <td>0.007994</td>\n",
       "      <td>0.007932</td>\n",
       "      <td>0.475464</td>\n",
       "      <td>0.532570</td>\n",
       "      <td>0.404769</td>\n",
       "      <td>0.095433</td>\n",
       "      <td>0.323472</td>\n",
       "      <td>0.014516</td>\n",
       "      <td>0.080062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16363 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.093088  0.443677  0.110506 -0.342519  0.334282  0.134133 -0.079811   \n",
       "1     -0.220892 -0.028724  0.146016 -0.145213  0.571359 -1.157924  0.182461   \n",
       "2      0.011180 -0.425624  0.102492 -0.452972 -0.219800 -0.457667  0.314573   \n",
       "3      0.177452  0.284411  0.059978  0.294704 -0.461114 -0.078591  0.027912   \n",
       "4     -0.104326  0.268305 -0.153165 -0.098395  0.136013 -0.023756  0.088492   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "16358  0.270939 -0.060949  0.430409  0.063049  0.087054  0.140644 -0.005588   \n",
       "16359  0.002887  0.125610  0.194917 -0.271436 -0.618391 -0.181236 -0.123064   \n",
       "16360  0.233436 -0.333714  0.249148 -0.065081 -0.009866  0.274120 -0.096856   \n",
       "16361  0.060705  0.650956  0.364366 -0.137743  0.125858  0.377062  0.200909   \n",
       "16362  0.397758  0.152827  0.053000  0.076836 -0.368298 -0.017233  0.010598   \n",
       "\n",
       "            7         8         9    ...       758       759       760  \\\n",
       "0      0.109643 -0.126565  0.068254  ...  0.244574  0.369231  0.375811   \n",
       "1     -0.330796  0.137217  0.456897  ...  0.123236 -0.210848  0.203079   \n",
       "2     -0.382062 -0.002846 -0.448729  ... -0.367298 -0.181243  0.003006   \n",
       "3     -0.055785  0.103038 -0.375385  ... -0.141730  0.011023 -0.028931   \n",
       "4     -0.139561 -0.056441 -0.164708  ... -0.252163 -0.168705  0.131596   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "16358 -0.014261 -0.410215 -0.261869  ...  0.041894 -0.191259  0.005462   \n",
       "16359  0.412982 -0.064641 -0.461746  ...  0.077754 -0.076400 -0.029041   \n",
       "16360  0.000036 -0.101609 -0.074891  ...  0.064156  0.152942  0.003013   \n",
       "16361 -0.164970  0.192967  0.105727  ... -0.133314 -0.485834 -0.629914   \n",
       "16362 -0.227857 -0.081790 -0.110883  ...  0.380262  0.007994  0.007932   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "0      0.209643 -0.253997  0.118141 -0.014578  0.338187  0.189731 -0.100272  \n",
       "1     -0.147492 -0.024679  0.178868  0.277888 -0.279467 -0.325976  0.090958  \n",
       "2     -0.251850 -0.453673  0.200214 -0.296771 -0.072697  0.017986 -0.251559  \n",
       "3      0.275780  0.392628  0.255509 -0.228360 -0.115353 -0.060927 -0.111012  \n",
       "4      0.134020 -0.135192  0.136525  0.146635 -0.221401 -0.097489  0.133560  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "16358  0.223951 -0.096526  0.615836  0.522237  0.275737  0.338247 -0.043370  \n",
       "16359  0.303071 -0.178294  0.099713  0.020586 -0.142542  0.411557 -0.139364  \n",
       "16360  0.009000 -0.168976  0.035944 -0.074668 -0.267087 -0.333665  0.200270  \n",
       "16361  0.162492 -0.074942  0.006419  0.090930  0.374194  0.108635  0.091865  \n",
       "16362  0.475464  0.532570  0.404769  0.095433  0.323472  0.014516  0.080062  \n",
       "\n",
       "[16363 rows x 768 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b892906",
   "metadata": {},
   "source": [
    "## Train logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "546cc397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.220891997,\n",
       " -0.0287244469,\n",
       " 0.146015704,\n",
       " -0.145213276,\n",
       " 0.571359217,\n",
       " -1.15792382,\n",
       " 0.182461426,\n",
       " -0.330795884,\n",
       " 0.137217268,\n",
       " 0.456897318,\n",
       " -0.208934724,\n",
       " -0.224243373,\n",
       " -0.206956863,\n",
       " 0.395993799,\n",
       " -0.489267319,\n",
       " -0.264985532,\n",
       " -0.228866264,\n",
       " -0.177542627,\n",
       " 0.360084951,\n",
       " -0.00257473439,\n",
       " -0.11394231,\n",
       " -0.201612964,\n",
       " -0.264656901,\n",
       " -0.398159206,\n",
       " 0.343744338,\n",
       " 0.128893048,\n",
       " 0.273463935,\n",
       " -0.11618045,\n",
       " -0.389896572,\n",
       " 0.0913909823,\n",
       " -0.116113514,\n",
       " 0.0094199758,\n",
       " -0.0121824145,\n",
       " 0.303918958,\n",
       " 0.460434675,\n",
       " -0.285305887,\n",
       " 0.286740899,\n",
       " 0.172171384,\n",
       " 0.206737384,\n",
       " 0.0589534417,\n",
       " 0.298478335,\n",
       " -0.0425849259,\n",
       " -0.00463876128,\n",
       " 0.206868067,\n",
       " -0.0282696187,\n",
       " -0.221420452,\n",
       " 0.149338931,\n",
       " -0.655038595,\n",
       " 0.0675567761,\n",
       " -0.134112984,\n",
       " 0.148336649,\n",
       " -0.0656467229,\n",
       " 0.353059173,\n",
       " 0.130215049,\n",
       " 0.126596659,\n",
       " 0.065672785,\n",
       " 0.592592597,\n",
       " 0.234645143,\n",
       " 0.203026175,\n",
       " -0.137577802,\n",
       " -0.0996464342,\n",
       " 0.0167466179,\n",
       " 0.481086344,\n",
       " -0.0798773393,\n",
       " -0.0738785416,\n",
       " -0.0964927524,\n",
       " -0.236611515,\n",
       " -0.127615482,\n",
       " 0.27068162,\n",
       " -0.214668572,\n",
       " -0.352340788,\n",
       " -0.243335485,\n",
       " -0.205503792,\n",
       " 0.338450193,\n",
       " -0.378232062,\n",
       " -0.376244456,\n",
       " -0.146457791,\n",
       " 0.215426147,\n",
       " -0.214227125,\n",
       " -0.0547479019,\n",
       " -0.103822798,\n",
       " -0.234286949,\n",
       " -0.473858207,\n",
       " -0.118234649,\n",
       " 0.207811117,\n",
       " 0.01407375,\n",
       " -0.0148333162,\n",
       " -0.412478566,\n",
       " 0.117335528,\n",
       " 0.00662632287,\n",
       " -0.183013588,\n",
       " -0.235846072,\n",
       " -0.313478947,\n",
       " -0.397682875,\n",
       " -0.0188253075,\n",
       " 0.713801444,\n",
       " -0.198597044,\n",
       " -0.125775427,\n",
       " -0.174957722,\n",
       " -0.0635218024,\n",
       " -0.231575251,\n",
       " 0.162663966,\n",
       " 0.453154862,\n",
       " 0.00767050683,\n",
       " 0.277094245,\n",
       " 0.332608581,\n",
       " -0.281540275,\n",
       " -0.382706106,\n",
       " -0.191961735,\n",
       " 0.230967641,\n",
       " 0.00667437911,\n",
       " -0.0280992463,\n",
       " 0.121113151,\n",
       " -0.0771064013,\n",
       " 0.245936736,\n",
       " -0.0710939616,\n",
       " 0.133639127,\n",
       " 0.158914313,\n",
       " -0.101978362,\n",
       " 0.027929008,\n",
       " 0.084541969,\n",
       " 0.327182174,\n",
       " 0.145687699,\n",
       " 0.430592179,\n",
       " 0.496615589,\n",
       " -0.185137957,\n",
       " 0.141227677,\n",
       " -0.303882778,\n",
       " 0.138109565,\n",
       " -0.22772485,\n",
       " 0.307444572,\n",
       " -0.291402996,\n",
       " -0.219836891,\n",
       " -0.104312599,\n",
       " -0.411863208,\n",
       " 0.352283418,\n",
       " 0.167136356,\n",
       " 0.152810976,\n",
       " 0.356793672,\n",
       " 0.14757067,\n",
       " 0.663766921,\n",
       " -0.110018782,\n",
       " 0.273011297,\n",
       " 0.00610754266,\n",
       " 0.657056868,\n",
       " 0.0337063223,\n",
       " 0.162058443,\n",
       " -0.078879267,\n",
       " 0.386876374,\n",
       " 0.227252141,\n",
       " 0.0847495198,\n",
       " -0.0497870818,\n",
       " 0.381425709,\n",
       " 0.185645327,\n",
       " -0.0160846524,\n",
       " 0.250923336,\n",
       " 0.406301141,\n",
       " 0.350078255,\n",
       " 0.264617562,\n",
       " -0.127670348,\n",
       " 0.417420357,\n",
       " -0.0747274756,\n",
       " -0.00342204305,\n",
       " 0.0656268895,\n",
       " -0.127220228,\n",
       " -0.405106127,\n",
       " -0.546945155,\n",
       " 0.0108139738,\n",
       " -0.209274992,\n",
       " -0.125812396,\n",
       " 0.32974714,\n",
       " 0.0590301678,\n",
       " 0.0950062871,\n",
       " 0.0776770338,\n",
       " 0.114765957,\n",
       " 0.166134,\n",
       " -0.0592771247,\n",
       " -0.145364821,\n",
       " -0.0825698227,\n",
       " -0.123515949,\n",
       " -0.413887501,\n",
       " 0.0737983659,\n",
       " -0.14234513,\n",
       " 0.313469738,\n",
       " -0.0702657253,\n",
       " 0.201992035,\n",
       " 0.182704195,\n",
       " -0.0443318114,\n",
       " 0.0523810722,\n",
       " 0.622826397,\n",
       " -0.214588374,\n",
       " -0.0671365485,\n",
       " 0.0402443409,\n",
       " -0.398671657,\n",
       " 0.112213731,\n",
       " -0.132945061,\n",
       " -0.311449289,\n",
       " 0.061097268,\n",
       " 0.114097178,\n",
       " 0.109291494,\n",
       " 0.266918004,\n",
       " 0.216982722,\n",
       " -0.25225389,\n",
       " -0.094276309,\n",
       " -0.22386682,\n",
       " -0.157715827,\n",
       " 0.190215394,\n",
       " 0.0321273692,\n",
       " 0.0573672578,\n",
       " -0.547111869,\n",
       " 0.275355548,\n",
       " -0.200956568,\n",
       " -0.320601612,\n",
       " -0.0786221102,\n",
       " 0.0954839736,\n",
       " 0.189926833,\n",
       " -0.287926435,\n",
       " -0.0494256653,\n",
       " 0.159424603,\n",
       " 0.0850945562,\n",
       " -0.0475899242,\n",
       " -0.661441445,\n",
       " 0.295844704,\n",
       " 0.0291501135,\n",
       " 0.270149171,\n",
       " 0.35943234,\n",
       " -0.0877078921,\n",
       " -0.0624814034,\n",
       " 0.248245731,\n",
       " -0.163029477,\n",
       " 0.060147766,\n",
       " -0.0737857223,\n",
       " 0.157022104,\n",
       " 0.105648488,\n",
       " 0.0522865355,\n",
       " -0.182066947,\n",
       " 0.0946904421,\n",
       " 0.161839902,\n",
       " -0.101915777,\n",
       " 0.00452189147,\n",
       " 0.0479739606,\n",
       " -0.11695075,\n",
       " 0.0939256996,\n",
       " -0.36856401,\n",
       " -0.185970023,\n",
       " -0.200746149,\n",
       " 0.0543790013,\n",
       " 1.01814115,\n",
       " -0.109879695,\n",
       " -0.291031659,\n",
       " 0.0676698089,\n",
       " -0.0380466804,\n",
       " 0.00435993262,\n",
       " 0.220321238,\n",
       " 0.202210635,\n",
       " -0.0613126531,\n",
       " -0.0390177369,\n",
       " -0.109867819,\n",
       " 0.256154954,\n",
       " 0.404208511,\n",
       " -0.000473633409,\n",
       " 0.084634617,\n",
       " 0.0804168358,\n",
       " -0.121148773,\n",
       " 0.581695199,\n",
       " -0.396509349,\n",
       " 0.0752328783,\n",
       " -0.387581438,\n",
       " 0.222649336,\n",
       " 0.0179461911,\n",
       " 0.279069245,\n",
       " -0.0572707281,\n",
       " -0.00300875306,\n",
       " -0.0445778333,\n",
       " 0.180603087,\n",
       " -0.280101538,\n",
       " -0.0936378166,\n",
       " -0.328080803,\n",
       " 0.565296769,\n",
       " -0.273749769,\n",
       " 0.227544338,\n",
       " 0.146969959,\n",
       " 0.46366024,\n",
       " 0.239566207,\n",
       " -0.0754612535,\n",
       " 0.00813146308,\n",
       " -0.443715036,\n",
       " 0.461761594,\n",
       " 0.314546108,\n",
       " 0.14432697,\n",
       " 0.169560134,\n",
       " 0.65748179,\n",
       " -0.0271551963,\n",
       " 0.0458043218,\n",
       " -0.340780467,\n",
       " 0.209567338,\n",
       " -0.034769468,\n",
       " -0.169924289,\n",
       " 0.426983565,\n",
       " -0.0311135314,\n",
       " 0.289842427,\n",
       " 0.40183723,\n",
       " 0.261001736,\n",
       " -0.249876767,\n",
       " -0.233015567,\n",
       " 0.0721873343,\n",
       " -0.374500334,\n",
       " 0.388008833,\n",
       " -0.261763841,\n",
       " 0.525306225,\n",
       " 0.189364702,\n",
       " 0.238240719,\n",
       " -0.122143552,\n",
       " 0.0406262092,\n",
       " 0.127752438,\n",
       " 0.0608241633,\n",
       " 0.135931641,\n",
       " -0.23415716,\n",
       " 0.156948686,\n",
       " 0.165632725,\n",
       " 0.0506609119,\n",
       " 0.138603836,\n",
       " -0.192972362,\n",
       " -0.123181693,\n",
       " -0.0207238719,\n",
       " 0.0234767348,\n",
       " 0.16451931,\n",
       " 0.0577321239,\n",
       " 0.0630672723,\n",
       " -0.11512062,\n",
       " -0.922051609,\n",
       " -0.381908357,\n",
       " -0.125855699,\n",
       " -0.35371536,\n",
       " -0.00568397343,\n",
       " 0.20881325,\n",
       " 0.228206292,\n",
       " 0.00394200534,\n",
       " -0.0593595058,\n",
       " 0.0082025528,\n",
       " -0.189160541,\n",
       " -0.0678738356,\n",
       " 0.352449119,\n",
       " 0.284361452,\n",
       " 0.00622551516,\n",
       " 0.0527391545,\n",
       " 0.145109698,\n",
       " 0.137643784,\n",
       " -0.549342811,\n",
       " 0.038118165,\n",
       " 0.022599481,\n",
       " -0.076568,\n",
       " -0.250759572,\n",
       " 0.0847868249,\n",
       " -0.163908303,\n",
       " 0.332047462,\n",
       " 0.0508522689,\n",
       " -0.562082291,\n",
       " -0.0648845583,\n",
       " -0.400882542,\n",
       " 0.332738996,\n",
       " -0.193832502,\n",
       " 0.114997551,\n",
       " 0.142767131,\n",
       " -0.0114034824,\n",
       " -0.182907149,\n",
       " 0.0708328411,\n",
       " -0.0237796083,\n",
       " -0.114937663,\n",
       " 0.0768569037,\n",
       " -0.166927025,\n",
       " -0.435108989,\n",
       " 0.408083856,\n",
       " -0.296510041,\n",
       " -0.202022016,\n",
       " -0.153052405,\n",
       " -0.533783674,\n",
       " 0.0529590845,\n",
       " 0.234396458,\n",
       " 0.0747368857,\n",
       " 0.163113266,\n",
       " 0.0184147693,\n",
       " -0.0800045878,\n",
       " 0.000554449856,\n",
       " -0.268221349,\n",
       " -0.0336585194,\n",
       " -0.157014072,\n",
       " 0.00379639771,\n",
       " -0.0563355908,\n",
       " 0.0424123704,\n",
       " -0.0488962941,\n",
       " 0.0866952613,\n",
       " 0.523033559,\n",
       " 0.0678531826,\n",
       " 0.178764313,\n",
       " -0.0258630626,\n",
       " -0.142833978,\n",
       " 0.690391541,\n",
       " 0.192607626,\n",
       " 0.149863899,\n",
       " 0.222501263,\n",
       " -0.088658601,\n",
       " 0.181512341,\n",
       " 0.702122748,\n",
       " 0.240494654,\n",
       " -0.289743125,\n",
       " 0.0180160701,\n",
       " -0.0802805349,\n",
       " -0.0859432891,\n",
       " 0.179637864,\n",
       " 0.0481803119,\n",
       " -0.126171768,\n",
       " 0.167326942,\n",
       " 0.234587222,\n",
       " 0.361290544,\n",
       " -0.352074146,\n",
       " -0.193960875,\n",
       " -0.634888649,\n",
       " -0.157486051,\n",
       " 0.406574011,\n",
       " -0.292659104,\n",
       " 0.0908556432,\n",
       " -0.144624114,\n",
       " -0.340921015,\n",
       " -0.167176843,\n",
       " 0.260442764,\n",
       " 0.194200113,\n",
       " -0.278422475,\n",
       " -0.0931241363,\n",
       " 0.0413454622,\n",
       " -0.140355349,\n",
       " 0.196815044,\n",
       " 0.071286723,\n",
       " 0.0502962433,\n",
       " 0.198412448,\n",
       " 0.0112264305,\n",
       " 0.117735192,\n",
       " -0.134916127,\n",
       " -0.0294317901,\n",
       " -0.382536113,\n",
       " 0.163767681,\n",
       " 0.071429804,\n",
       " 0.147515938,\n",
       " -0.17231366,\n",
       " -0.144097432,\n",
       " 0.163876355,\n",
       " 0.218468726,\n",
       " -0.0822218508,\n",
       " -0.0270913281,\n",
       " -0.700218916,\n",
       " -0.132771522,\n",
       " -0.36304158,\n",
       " 0.221873403,\n",
       " -0.108081251,\n",
       " -0.0237354897,\n",
       " 0.0304553732,\n",
       " 0.0276051573,\n",
       " 0.187866613,\n",
       " -0.259172052,\n",
       " -0.20447804,\n",
       " -0.00673119724,\n",
       " -0.142611146,\n",
       " 0.0499357767,\n",
       " -0.0143666416,\n",
       " 0.0397779159,\n",
       " -0.0373386145,\n",
       " 0.142408267,\n",
       " 0.109177455,\n",
       " -0.0218510944,\n",
       " 0.0520587675,\n",
       " -0.118322022,\n",
       " -0.290681094,\n",
       " 0.183958322,\n",
       " 0.0970003754,\n",
       " 0.0993810594,\n",
       " -0.0282088481,\n",
       " -0.221520767,\n",
       " -0.127213925,\n",
       " 0.171727031,\n",
       " 0.417230666,\n",
       " 0.151752979,\n",
       " -0.071009025,\n",
       " -0.11165534,\n",
       " -0.0132556967,\n",
       " -0.191654682,\n",
       " 0.333152682,\n",
       " 0.341189921,\n",
       " 0.466380388,\n",
       " 0.148687974,\n",
       " -0.242749065,\n",
       " -0.0698106661,\n",
       " 0.315146506,\n",
       " 0.0728018731,\n",
       " -0.385177612,\n",
       " 0.521333814,\n",
       " 0.386695385,\n",
       " 0.236222416,\n",
       " 0.222766876,\n",
       " 0.300194502,\n",
       " 0.0605693907,\n",
       " 0.0352463722,\n",
       " 0.0137922913,\n",
       " -0.0213348642,\n",
       " 0.200697005,\n",
       " -0.0661892891,\n",
       " 0.113991812,\n",
       " -0.0430408344,\n",
       " 0.118779756,\n",
       " 0.059212625,\n",
       " -0.00225207955,\n",
       " 0.00214774162,\n",
       " -0.0139356647,\n",
       " -0.194209531,\n",
       " 0.0782416612,\n",
       " -0.285397947,\n",
       " 0.173261091,\n",
       " -0.601257503,\n",
       " 0.00307745114,\n",
       " -0.26295203,\n",
       " -0.494315565,\n",
       " -0.0804551616,\n",
       " 0.0899595916,\n",
       " -0.00673597306,\n",
       " 0.122405507,\n",
       " 0.158609197,\n",
       " 0.0627711341,\n",
       " -0.0934326649,\n",
       " 0.42953217,\n",
       " 0.20052816,\n",
       " 0.237493992,\n",
       " 0.250056863,\n",
       " 0.137333199,\n",
       " 0.0608423874,\n",
       " 0.259553879,\n",
       " 0.112366006,\n",
       " 0.184122264,\n",
       " 0.255119801,\n",
       " 0.169628546,\n",
       " 0.446548104,\n",
       " 0.134259611,\n",
       " -0.179926828,\n",
       " -0.0621771663,\n",
       " 0.233884454,\n",
       " -0.165858358,\n",
       " 0.250384748,\n",
       " -0.364901096,\n",
       " 0.125222728,\n",
       " 0.196056306,\n",
       " 0.0619686544,\n",
       " 0.0696114004,\n",
       " 0.245227367,\n",
       " -0.0882113352,\n",
       " -0.122103587,\n",
       " -0.506048381,\n",
       " 0.164622486,\n",
       " -0.208262801,\n",
       " -0.16814667,\n",
       " 0.175845653,\n",
       " 0.269022942,\n",
       " 0.219130605,\n",
       " 0.129974231,\n",
       " -0.345774174,\n",
       " -0.0625107065,\n",
       " 0.0758037046,\n",
       " 0.469771296,\n",
       " -0.168496728,\n",
       " -0.0825335085,\n",
       " -0.202923119,\n",
       " 0.151705414,\n",
       " 0.0150449947,\n",
       " -0.948006094,\n",
       " 0.160404012,\n",
       " -0.139242932,\n",
       " -0.26742965,\n",
       " -0.101688683,\n",
       " 0.0556547605,\n",
       " -0.0249342099,\n",
       " 0.429915637,\n",
       " -0.18507193,\n",
       " 0.40193814,\n",
       " -0.0889032483,\n",
       " -0.295788735,\n",
       " 0.116583519,\n",
       " 0.278412044,\n",
       " -0.0699446052,\n",
       " 0.122457378,\n",
       " 0.502650321,\n",
       " -0.282052338,\n",
       " 0.211022362,\n",
       " -0.0534970909,\n",
       " -0.0531800985,\n",
       " 0.00620021671,\n",
       " -0.00542871654,\n",
       " 0.0501920506,\n",
       " 0.0069631096,\n",
       " 0.257066786,\n",
       " 0.102485754,\n",
       " -0.213311359,\n",
       " 0.106058218,\n",
       " -0.205042243,\n",
       " -0.0206783414,\n",
       " 0.225140393,\n",
       " -0.0642033145,\n",
       " 0.154568106,\n",
       " -0.268681586,\n",
       " -0.168358237,\n",
       " -0.062249735,\n",
       " 0.238672182,\n",
       " 0.219341904,\n",
       " 0.655062616,\n",
       " -0.197183222,\n",
       " -0.158072904,\n",
       " -0.286810398,\n",
       " 0.273061186,\n",
       " -0.15607442,\n",
       " -0.141626894,\n",
       " 0.101316586,\n",
       " 0.0453212187,\n",
       " 0.0490615927,\n",
       " 0.774932623,\n",
       " -0.0281549953,\n",
       " -0.0591771342,\n",
       " -0.174749076,\n",
       " 0.373956919,\n",
       " 0.509774566,\n",
       " 0.0379568152,\n",
       " 0.257721603,\n",
       " -0.215812594,\n",
       " 0.00197346509,\n",
       " -0.0103163831,\n",
       " 0.197544694,\n",
       " -0.0994027406,\n",
       " -0.0553504974,\n",
       " 0.172131225,\n",
       " -0.448465526,\n",
       " -0.142316848,\n",
       " 0.00581711158,\n",
       " 0.305731505,\n",
       " 0.140894324,\n",
       " -0.209011629,\n",
       " 0.660886586,\n",
       " 0.505746484,\n",
       " 0.345940202,\n",
       " 0.194441557,\n",
       " -0.26587835,\n",
       " 0.0539637655,\n",
       " 0.158583298,\n",
       " -0.371745169,\n",
       " -0.165473163,\n",
       " 0.0951754078,\n",
       " 0.022547856,\n",
       " 0.0124926418,\n",
       " -0.311818659,\n",
       " -0.174618632,\n",
       " -0.279087514,\n",
       " 0.187373936,\n",
       " 0.0299875755,\n",
       " 0.132143721,\n",
       " -0.0677428246,\n",
       " 0.00362572074,\n",
       " 0.0403340086,\n",
       " -0.194107652,\n",
       " 0.081543684,\n",
       " -0.269762695,\n",
       " 0.0431681052,\n",
       " 0.0622180887,\n",
       " -0.0187640414,\n",
       " -0.18736133,\n",
       " -0.306992799,\n",
       " -0.174203277,\n",
       " 0.198256046,\n",
       " -0.358264148,\n",
       " 0.0954592153,\n",
       " 0.256561011,\n",
       " -0.261060655,\n",
       " -0.164994821,\n",
       " -0.0295919105,\n",
       " -0.0310137272,\n",
       " -0.290481508,\n",
       " -0.272716939,\n",
       " -0.198206723,\n",
       " 0.322712779,\n",
       " 0.123899169,\n",
       " -0.142366692,\n",
       " 0.401917279,\n",
       " -0.13180615,\n",
       " 0.0253786854,\n",
       " -2.61813402e-05,\n",
       " -0.640439212,\n",
       " -0.188452646,\n",
       " 0.0871911794,\n",
       " 0.283123076,\n",
       " 0.247864872,\n",
       " -0.37590903,\n",
       " -0.219113186,\n",
       " 0.662867904,\n",
       " -0.0752318352,\n",
       " -0.0782509446,\n",
       " 0.119165421,\n",
       " -0.462668896,\n",
       " 0.0379967429,\n",
       " 0.173814386,\n",
       " 0.593920171,\n",
       " -0.047339581,\n",
       " 0.0534719899,\n",
       " 0.113071673,\n",
       " -0.0457517058,\n",
       " -0.239605784,\n",
       " 0.112383381,\n",
       " 0.23225233,\n",
       " -0.252757758,\n",
       " -0.0245859027,\n",
       " -0.19098863,\n",
       " 0.0335281007,\n",
       " -0.239732116,\n",
       " 0.049572207,\n",
       " 0.313148409,\n",
       " -0.152763382,\n",
       " 0.430158973,\n",
       " -0.0739189088,\n",
       " 0.245784,\n",
       " -0.402497381,\n",
       " -0.145918787,\n",
       " 0.136529535,\n",
       " -0.386918724,\n",
       " 0.221182182,\n",
       " 0.147206247,\n",
       " -0.309630871,\n",
       " 0.569072962,\n",
       " -0.504755855,\n",
       " 0.0264290795,\n",
       " 0.323864758,\n",
       " -0.210438728,\n",
       " 0.363211364,\n",
       " 0.292418659,\n",
       " -0.139975742,\n",
       " 0.128805578,\n",
       " 0.00474603474,\n",
       " -0.130065098,\n",
       " -0.510177374,\n",
       " -0.303997636,\n",
       " -0.234962493,\n",
       " -0.199495479,\n",
       " 0.0850817859,\n",
       " -0.0371901244,\n",
       " 0.153421685,\n",
       " -0.198428512,\n",
       " 0.229918629,\n",
       " -0.303818911,\n",
       " 0.0271154791,\n",
       " 0.556239307,\n",
       " -0.173433602,\n",
       " 0.515862584,\n",
       " 0.0639429763,\n",
       " 0.135649264,\n",
       " -0.0272373687,\n",
       " 0.089082256,\n",
       " -0.0966467112,\n",
       " 0.123236127,\n",
       " -0.21084775,\n",
       " 0.203078866,\n",
       " -0.147492066,\n",
       " -0.0246789679,\n",
       " 0.178867668,\n",
       " 0.27788797,\n",
       " -0.279467493,\n",
       " -0.325976074,\n",
       " 0.0909575969]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a60dc73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0cad0f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0]) == len(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "227cfa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/class/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression() #random_state=1\n",
    "clf_log_reg = log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cdf03e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8875649251451267\n"
     ]
    }
   ],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = clf_log_reg.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f831efc",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sentiment = clf_log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf300c2",
   "metadata": {},
   "source": [
    "# Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1fa24",
   "metadata": {},
   "source": [
    "Download the lexicon from http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar and extract it into `data/positive-words.txt` and `data/negative-words.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00150f",
   "metadata": {},
   "source": [
    "The following pre-processing steps are inspired from https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b17b55",
   "metadata": {},
   "source": [
    "We also pre-processed data so that it begins with < s> tokens (and ends with < /s> tokens). Inspired from answer: https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b0935",
   "metadata": {},
   "source": [
    "normalize text to regular expression\n",
    "code from https://gist.github.com/yamanahlawat/4443c6e9e65e74829dbb6b47dd81764a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37554074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
