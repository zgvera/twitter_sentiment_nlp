{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0640dc4",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Text\n",
    "\n",
    "In today’s world, Twitter provides people with a way to publicly express their thoughts on any given subject in a concise, condensed format. This allows us to use tweets as a way to predict users’ thoughts or feelings on a certain subject.\n",
    "\n",
    "Since the 2016 U.S. election, the influence of social media on society has become more and more concerning. Fake news, hate speech, polarization, and echo chambers attract growing scholarships to pay attention to the discussions in the online space. Understanding the sentimental content on social media is crucial to further analysis\n",
    "\n",
    "In this project, we are going to compare and contrast two models on the performance of classifying a tweet based on sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d54aa",
   "metadata": {},
   "source": [
    "## Load data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2b38e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/zhenguo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import your libraries here\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "# from nltk.stem import SnowballStemmer\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8de6b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_patterns = [\n",
    "  (r'won\\'t', 'will not'),\n",
    "  (r'can\\'t', 'cannot'),\n",
    "  (r'i\\'m', 'i am'),\n",
    "  (r'ain\\'t', 'is not'),\n",
    "  (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "  (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "  (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "  (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "  (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "  (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in replacement_patterns]\n",
    "\n",
    "def replace(text):\n",
    "    s = text\n",
    "    for (pattern, repl) in patterns:\n",
    "        s = re.sub(pattern, repl, s)\n",
    "    return s\n",
    "\n",
    "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Process the paragram so it is tokenized into sentences.\n",
    "    To keep the nuance of social media, we are keeping the punctuation and forms of words.\n",
    "    \"\"\"\n",
    "    sent_text = nltk.sent_tokenize(text) # this gives us a list of sentences\n",
    "    \n",
    "    # now loop over each sentence and tokenize it separately\n",
    "    s = []\n",
    "    for sentence in sent_text:\n",
    "        # regualr expression\n",
    "        sentence = replace(sentence)\n",
    "        # tokenize sentence\n",
    "        tokenized_text = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
    "\n",
    "        s = s + tokenized_text\n",
    "    return s\n",
    "\n",
    "def process_data(series):\n",
    "    # returns text in this format:\n",
    "    # data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "    # \t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "    # \t\t\t['yet', 'another', 'sentence'],\n",
    "    # \t\t\t['one', 'more', 'sentence'],\n",
    "    # \t\t\t['and', 'the', 'final', 'sentence']]\n",
    "    tweets = []\n",
    "    for _,row in series.items():\n",
    "        tweets.append(process_text(str(row)))\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e17996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet = pd.read_csv('data/Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23763a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27479  ed167662a5                         But it was worth it  ****.   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "0                    I`d have responded, if I were going   neutral  \n",
       "1                                               Sooo SAD  negative  \n",
       "2                                            bullying me  negative  \n",
       "3                                         leave me alone  negative  \n",
       "4                                          Sons of ****,  negative  \n",
       "...                                                  ...       ...  \n",
       "27476                                             d lost  negative  \n",
       "27477                                      , don`t force  negative  \n",
       "27478                          Yay good for both of you.  positive  \n",
       "27479                         But it was worth it  ****.  positive  \n",
       "27480  All this flirting going on - The ATG smiles. Y...   neutral  \n",
       "\n",
       "[27481 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "121da1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = process_data(df_tweet['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c560ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8044cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From notebook 11\n",
    "def load_lexicon(filename):\n",
    "    \"\"\"\n",
    "    Load a file from Bing Liu's sentiment lexicon\n",
    "    (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), containing\n",
    "    English words in Latin-1 encoding.\n",
    "    \n",
    "    One file contains a list of positive words, and the other contains\n",
    "    a list of negative words. The files contain comment lines starting\n",
    "    with ';' and blank lines, which should be skipped.\n",
    "    \"\"\"\n",
    "    lexicon = []\n",
    "    with open(filename, encoding='latin-1') as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip()\n",
    "            if line and not line.startswith(';'):\n",
    "                lexicon.append(line)\n",
    "    return lexicon\n",
    "\n",
    "pos_words = load_lexicon('data/positive-words.txt')\n",
    "neg_words = load_lexicon('data/negative-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17723d58",
   "metadata": {},
   "source": [
    "## Train the embeddings\n",
    "\n",
    "Right now using Glovec, can be changed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fb5f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02e707a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From notebook 11\n",
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
    "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
    "    whether there is an initial line with the dimensions of the matrix.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
    "\n",
    "# for better performance, use the 42B data https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "embeddings = load_embeddings('data/glove.6B.50d.txt')\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68998604",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vectors = embeddings.loc[embeddings.index.isin(pos_words)].dropna()\n",
    "neg_vectors = embeddings.loc[embeddings.index.isin(neg_words)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc2b800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b892906",
   "metadata": {},
   "source": [
    "## Train logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a59d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3625dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee9f5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(loss='log', random_state=0, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f719cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    model.fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d40655bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecs_to_sentiment(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment and return it.\n",
    "\n",
    "    return [p[1]-p[0] for p in predictions]\n",
    "\n",
    "def words_to_sentiment(words):\n",
    "    vecs = embeddings.loc[embeddings.index.isin(words)].dropna()\n",
    "    # if word is in embedding list\n",
    "    if vecs.empty:\n",
    "        log_odds = 0\n",
    "    else:\n",
    "        log_odds = vecs_to_sentiment(vecs)\n",
    "        \n",
    "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
    "\n",
    "def text_to_sentiment(tweet_tok):\n",
    "    # Compute sentiments for all the tokens and return the mean value\n",
    "    \n",
    "    sentiment = words_to_sentiment(tweet_tok)\n",
    "    return sentiment['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2785224a",
   "metadata": {},
   "source": [
    "### Predict label using logistic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "29f4973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_predict(tweets):\n",
    "    log_labels = []\n",
    "    i = 0\n",
    "    for tweet in tweets:\n",
    "#         print(i)\n",
    "        score = text_to_sentiment(tweet)\n",
    "        if score > 0:\n",
    "            log_labels.append('positive')\n",
    "        elif score < 0:\n",
    "            log_labels.append('negative')\n",
    "        else:\n",
    "            log_labels.append('neutral')\n",
    "        i += 1\n",
    "    return log_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "af7f27b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sentiment = log_predict(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0d9da7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f831efc",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7f7a1869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the precision for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "          gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "          predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double precision (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    TP, FP = 0, 0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if gold_labels[i] == predicted_labels[i]:\n",
    "            if predicted_labels[i] == '1':\n",
    "                TP += 1\n",
    "        else:\n",
    "            if predicted_labels[i] == '1':\n",
    "                FP += 1\n",
    "    return TP / (TP + FP) if (TP + FP) else 0\n",
    "\n",
    "\n",
    "def recall(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the recall for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "      gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "      predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double recall (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    TP, FN = 0, 0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if gold_labels[i] == predicted_labels[i]:\n",
    "            if predicted_labels[i] == '1':\n",
    "                TP += 1\n",
    "        else:\n",
    "            if predicted_labels[i] != '1':\n",
    "                FN += 1\n",
    "    return TP / (TP + FN) if (TP + FN) else 0\n",
    "\n",
    "\n",
    "def f1(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the f1 for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "      gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "      predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double f1 (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    P = precision(gold_labels, predicted_labels)\n",
    "    R = recall(gold_labels, predicted_labels)\n",
    "    return 2 * P * R / (P + R) if (P + R) else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "58c67954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27421"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "55eb2262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tweet[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "05d80b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p = precision(df_tweet[\"sentiment\"], tweet_sentiment)\n",
    "log_r = recall(df_tweet[\"sentiment\"], tweet_sentiment)\n",
    "log_f1 = f1(df_tweet[\"sentiment\"], tweet_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df8fe747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0, 0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_p, log_r,log_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf300c2",
   "metadata": {},
   "source": [
    "# Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1fa24",
   "metadata": {},
   "source": [
    "Download the lexicon from http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar and extract it into `data/positive-words.txt` and `data/negative-words.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00150f",
   "metadata": {},
   "source": [
    "The following pre-processing steps are inspired from https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b17b55",
   "metadata": {},
   "source": [
    "We also pre-processed data so that it begins with < s> tokens (and ends with < /s> tokens). Inspired from answer: https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b0935",
   "metadata": {},
   "source": [
    "normalize text to regular expression\n",
    "code from https://gist.github.com/yamanahlawat/4443c6e9e65e74829dbb6b47dd81764a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37554074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
